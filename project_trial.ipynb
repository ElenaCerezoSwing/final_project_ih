{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper', 'cardboard', 'plastic', 'metal', 'trash', 'glass']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = os.listdir('./data/input_images')\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picture_per_classes(classes):\n",
    "    return [os.listdir('./data/input_images/'+ item) for item in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_picture_per_classes(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_name(item):\n",
    "    return item.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_name = [get_image_name(item) for image in images for item in image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_from_images(item):\n",
    "    num = re.findall('\\d+', item)\n",
    "    return item.split(num[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_classes = [get_classes_from_images(item) for item in images_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {'Images_name':images_name, 'Image_class': image_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paper85</th>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper503</th>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper75</th>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper482</th>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper300</th>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Image_class\n",
       "paper85        paper\n",
       "paper503       paper\n",
       "paper75        paper\n",
       "paper482       paper\n",
       "paper300       paper"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.DataFrame(new_dict, index=images_name, columns=['Image_class'])\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_p = pd.get_dummies(p['Image_class'])\n",
    "new_p.head()\n",
    "new_p.to_csv(r'./images_dummies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths:\n",
    "DATASET = 'images_dummies.csv'\n",
    "ROOT_IMG = './data/input_images/'\n",
    "ROOT_IMG_DAUGHTERS = classes\n",
    "ROOT_MODELS = 'Models/'\n",
    "# img size:\n",
    "ROW_IMAGES = 384\n",
    "COL_IMAGES = 512\n",
    "IMG_COLORS = 3\n",
    "input_shape = (ROW_IMAGES, COL_IMAGES, IMG_COLORS)\n",
    "# num of pictures:\n",
    "PIC_NUM = 2527\n",
    "# train-test parameters:\n",
    "TRAIN_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "# fit parameters:\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS =10\n",
    "\n",
    "COLUMNS = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
    "num_classes = len(COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_p['image'] = new_p.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(item):\n",
    "    return item + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parental_path(item):\n",
    "    parental = get_classes_from_images(item)\n",
    "    return parental + '/'+ get_image(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_path_to_image(item):\n",
    "    imagen = cv2.imread(ROOT_IMG+get_parental_path(item))\n",
    "    return imagen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(item):\n",
    "    image = transform_path_to_image(item)\n",
    "    # Converts image to an inx * iny size, change color to greyscale and flatten it into 1D ndarray\n",
    "    image = cv2.resize(image,(256, 192)) \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "    image = np.reshape(image,(256, 192))\n",
    "    return np.ndarray.flatten(image)\n",
    "    #normalize = lambda pixel: pixel/255\n",
    "    #float_thirty_two = lambda elem : elem.astype('float32')\n",
    "    #return normalize(image_arr)\n",
    "\n",
    "    #return float_thirty_two(norm_arr)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240, 240, 239, ..., 195, 196, 197], dtype=uint8)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_p['image'] = new_p['image'].apply(image_to_array)\n",
    "new_p.head()\n",
    "new_p['image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def image_to_array(image,inx,iny):\n",
    "    # Converts image to an inx * iny size, change color to greyscale and flatten it into 1D ndarray\n",
    "    image = cv2.resize(image,(inx,iny)) \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "    image = np.reshape(image,(inx,iny))\n",
    "    return np.ndarray.flatten(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_p.image.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def transform_data(item):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COLUMNS = new_p.columns\n",
    "data = pd.DataFrame(new_p,columns=['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash', 'image'])\n",
    "data.reset_index(drop=True)\n",
    "data.to_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(df.image[0])\n",
    "imagen = cv2.imread(ROOT_IMG+'paper/paper1.jpg')\n",
    "for lista in imagen:\n",
    "    for pixel in lista:\n",
    "        print(pixel/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def replace_value(item):\n",
    "    return item.replace('\\n', ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imagen= cv2.imread('./data/input_images/paper/paper7.jpg')\n",
    "plt.imshow(Image.fromarray(imagen.squeeze()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_p.image.values, new_p.drop(columns=['image']), test_size=TRAIN_RATIO, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.uint8"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n",
      "255\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-6967829e5070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hola'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROW_IMAGES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOL_IMAGES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROW_IMAGES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOL_IMAGES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mROW_IMAGES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOL_IMAGES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Prepare data to feed the NN\n",
    "num_classes = 6\n",
    "\n",
    "#X_train = X_train.astype('int32')\n",
    "#X_test = X_test.astype('int32')\n",
    "#X_test /= 255\n",
    "# Ask keras which format to use depending on used backend and arrange data as expected\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, ROW_IMAGES/2, COL_IMAGES/2)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, ROW_IMAGES/2, COL_IMAGES/2)\n",
    "    input_shape = (1, ROW_IMAGES/2, COL_IMAGES/2)\n",
    "else:\n",
    "    print('hola')\n",
    "    print(X_train)\n",
    "    X_train = X_train.reshape(X_train.shape[0], ROW_IMAGES/2, COL_IMAGES/2, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], ROW_IMAGES/2, COL_IMAGES/2, 1)\n",
    "    input_shape = (ROW_IMAGES/2, COL_IMAGES/2, 1)\n",
    "# Incoming data is in uint8. Cast the input data images to be floats in range [0.0-1.0]\n",
    "#normalize = lambda pixel: pixel/255 \n",
    "#x_train = normalize(X_train)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "#X_test = normalize(X_test)\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the neural network proposed architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 10\n",
    "model.fit(X_train, y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         verbose=1,\n",
    "         validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
